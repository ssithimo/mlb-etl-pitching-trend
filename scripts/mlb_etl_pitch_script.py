# -*- coding: utf-8 -*-
"""mlb_etl_pitch_script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16nsrl7MGPkhnb2xuxNa5NBuCfw4mYx89
"""

import logging
import os
from datetime import datetime, timedelta
import pandas as pd
from pybaseball import statcast
from sqlalchemy import create_engine
from dotenv import load_dotenv

# === LOGGING CONFIGURATION === #
log_dir = os.path.expanduser('~/mlb-etl-logs')
os.makedirs(log_dir, exist_ok = True)

log_file = os.path.join(log_dir, 'etl_output.log')
dates_log_path = os.path.join(log_dir, 'etl_dates.log')

logging.basicConfig(
    filename = log_file,
    level = logging.INFO,
    format = '%(asctime)s - %(levelname)s - %(message)s',
    filemode = 'a'  # Explicitly append
)

# === DATE RANGE === #
def get_last_week_dates():
    today = datetime.today()
    end_date = today - timedelta(days=1)
    start_date = end_date - timedelta(days=6)
    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')

# === EXTRACT === #
def extract_data(start_date, end_date):
    logging.info(f"Fetching pitch data from {start_date} to {end_date}...")
    df = statcast(start_dt = start_date, end_dt = end_date)
    logging.info(f"Retrieved {len(df)} pitch records.")
    return df

# === CLEAN === #
def clean_and_select_columns(df):
    selected_cols = [
        'game_date', 'player_name', 'pitcher', 'batter', 'pitch_type', 'release_speed',
        'release_spin_rate', 'release_extension', 'p_throws', 'stand', 'inning', 'inning_topbot',
        'outs_when_up', 'balls', 'strikes', 'on_1b', 'on_2b', 'on_3b', 'at_bat_number', 'pitch_number',
        'zone', 'description', 'events', 'home_team', 'away_team', 'plate_x', 'plate_z',
        'release_pos_x', 'release_pos_y', 'release_pos_z', 'vx0', 'vy0', 'vz0', 'ax', 'ay', 'az',
        'fielder_2', 'bat_score', 'fld_score',
    ]
    pitch_df = df[selected_cols].copy()
    pitch_df['game_date'] = pd.to_datetime(pitch_df['game_date'])
    logging.info(f"Data cleaned. Remaining records: {len(pitch_df)}")
    return pitch_df

# === LOAD === #
def load_data_to_db(df):
    load_dotenv()
    DATABASE_URL = os.getenv("DATABASE_URL")
    if DATABASE_URL:
        engine = create_engine(DATABASE_URL)
        df.to_sql('pitch_data', engine, if_exists='append', index=False)
        logging.info("Data successfully loaded into PostgreSQL.")
    else:
        logging.warning("No DATABASE_URL found. Skipping database load.")

# === DATE LOGGING === #
def log_dates(start_date, end_date):
    with open(dates_log_path, 'a') as f:
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Extracted data from {start_date} to {end_date}\n")

# === MAIN === #
def main():
    logging.info("=== ETL Job Started ===")
    start_date, end_date = get_last_week_dates()
    log_dates(start_date, end_date)

    try:
        df = extract_data(start_date, end_date)
        cleaned_df = clean_and_select_columns(df)
        load_data_to_db(cleaned_df)
        logging.info("ETL Job Completed Successfully.")
    except Exception as e:
        logging.exception(f"ETL Job Failed: {e}")

if __name__ == "__main__":
    main()